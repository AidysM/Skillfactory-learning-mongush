{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c541d59-3f02-4d1d-8075-97b2704bdefb",
   "metadata": {},
   "source": [
    "Парсеры — это специальные программы, которые позволяют собирать информацию с веб-сайтов, не заходя на них через браузер. Т. е., например, если вы захотели составить базу данных товара какого-либо интернет-магазина, то вам не обязательно перемещаться по нему и вручную отбирать все названия, фото товара и ссылки на сам товар. Для этого достаточно написать парсер, который по определенным отличительным признакам в HTML-коде (как правило, это классы или id) будет находить вам нужную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b6fa24-6e81-4184-bc39-97ead21f4bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to Python.org']\n"
     ]
    }
   ],
   "source": [
    "import requests # импортируем наш знакомый модуль\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    " \n",
    " \n",
    " \n",
    "html = requests.get('https://www.python.org/').content # получим html главной странички официального сайта python\n",
    " \n",
    "tree = lxml.html.document_fromstring(html)\n",
    "title = tree.xpath('/html/head/title/text()') # забираем текст тега <title> из тега <head> который лежит в свою очередь внутри тега <html> (в этом невидимом теге <head> у нас хранится основная информация о страничке. Её название и инструкции по отображению в браузере. \n",
    " \n",
    "print(title) # выводим полученный заголовок страницы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a9b501-d8c3-441d-9e13-5c1644d66d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcoming Microsoft as a Visionary Sponsor\n"
     ]
    }
   ],
   "source": [
    "import requests # импортируем наш знакомый модуль\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    " \n",
    "# создадим объект ElementTree. Он возвращается функцией parse() \n",
    "tree = etree.parse('Welcome to Python.org.html', lxml.html.HTMLParser()) # попытаемся спарсить наш файл с помощью html парсера. Сам html - это то что мы скачали и поместили в папку из браузера.\n",
    " \n",
    "ul = tree.findall('body/div/div[3]/div/section/div[2]/div[1]/div/ul/li[1]') # помещаем в аргумент методу findall скопированный xpath. Здесь мы получим все элементы списка новостей. (Все заголовки и их даты)  \n",
    " \n",
    "# создаём цикл в котором мы будем выводить название каждого элемента из списка\n",
    "for li in ul:\n",
    "    a = li.find('a') # в каждом элементе находим где хранится заголовок новости. У нас это тег <a>. Т.е. гиперссылка на которую нужно нажать, чтобы перейти на страницу с новостью. (Гиперссылки в html это всегда тэг <a>)\n",
    "    print(a.text) # из этого тега забираем текст - это и будет нашим названием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a5b53b-8a4f-4bea-bc61-9608af77e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' MY TEXT ']\n"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    " \n",
    " \n",
    "html = ''' <html>\n",
    " <head> <title> Some title </title> </head>\n",
    " <body>\n",
    "  <tag1> some text\n",
    "     <tag2> MY TEXT </tag2>\n",
    "   </tag1>\n",
    " </body>\n",
    "</html>\n",
    "'''\n",
    " \n",
    "tree = lxml.html.document_fromstring(html)\n",
    " \n",
    "text = tree.xpath('/html/body/tag1/tag2/text()')\n",
    " \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9bf892-c4e0-419a-9913-eef7a73bcbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-29T16:20:00.000003+00:00 Welcoming Microsoft as a Visionary Sponsor\n"
     ]
    }
   ],
   "source": [
    "import requests # импортируем наш знакомый модуль\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    " \n",
    " \n",
    "html = requests.get('https://www.python.org/').content # получим html главной странички официального сайта python\n",
    " \n",
    "# создадим объект ElementTree. Он возвращается функцией parse() \n",
    "tree = etree.parse('Welcome to Python.org.html', lxml.html.HTMLParser()) # попытаемся спарсить наш файл с помощью html парсера\n",
    " \n",
    "ul = tree.findall('body/div/div[3]/div/section/div[2]/div[1]/div/ul/li[1]') # помещаем в аргумент методу findall скопированный xpath\n",
    " \n",
    "# создаём цикл в котором мы будем выводить название каждого элемента из списка\n",
    "for li in ul:\n",
    "    a = li.find('a') # в каждом элементе находим где хранится название. У нас это тег <a>\n",
    "    time = li.find('time')\n",
    "    print(time.get('datetime'), a.text) # из этого тега забираем текст - это и будет нашим названием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e050f5f-0a2c-4138-b514-0ef06ac26e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
